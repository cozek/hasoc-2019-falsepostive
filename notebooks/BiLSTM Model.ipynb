{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from pprint import pprint\n",
    "import io\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from nltk import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation,Bidirectional\n",
    "from keras.layers import Concatenate, Permute, Dot, Multiply,RepeatVector,add,Flatten,Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform,TruncatedNormal\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hasoc2019 as hasoc_utils\n",
    "import utils.preprocessing as preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_file = '../data/train/english_dataset.tsv',\n",
    "    fast_text_loc = \\\n",
    "        \"/Users/cozek/Documents/MTech/3rd Sem/Project/crawl-300d-2M-subword/crawl-300d-2M-subword.vec\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = hasoc_utils.open_data_as_df(args.data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_model(input_shape, embedding_layer):\n",
    "    \n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='glorot_normal',\n",
    "                           recurrent_regularizer=keras.regularizers.l2(0.001),activation='relu'),  \n",
    "                      merge_mode='mul')(embeddings)\n",
    "    \n",
    "    X = Dropout(0.5, noise_shape=None, seed=None)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1, \n",
    "                                        momentum=0.99, \n",
    "                                        epsilon=0.001, center=True,\n",
    "                                        scale=True, beta_initializer='zeros', \n",
    "                                        gamma_initializer='ones', \n",
    "                                        moving_mean_initializer='zeros', \n",
    "                                        moving_variance_initializer='ones', \n",
    "                                        beta_regularizer=None, \n",
    "                                        gamma_regularizer=None, \n",
    "                                        beta_constraint=None, gamma_constraint=None)(X)\n",
    "\n",
    "    X = Bidirectional(LSTM(128, kernel_initializer='glorot_normal',activation='relu',\n",
    "                      recurrent_regularizer=keras.regularizers.l2(0.001)),\n",
    "                      merge_mode='mul')(X)\n",
    "    \n",
    "    X = keras.layers.BatchNormalization(axis=-1, \n",
    "                                        momentum=0.99, \n",
    "                                        epsilon=0.001, center=True,\n",
    "                                        scale=True, beta_initializer='zeros', \n",
    "                                        gamma_initializer='ones', \n",
    "                                        moving_mean_initializer='zeros', \n",
    "                                        moving_variance_initializer='ones', \n",
    "                                        beta_regularizer=None, \n",
    "                                        gamma_regularizer=None, \n",
    "                                        beta_constraint=None, gamma_constraint=None)(X)\n",
    "    X = Dropout(0.2, noise_shape=None, seed=None)(X)\n",
    "    X = Dense(256)(X)\n",
    "    X = keras.layers.BatchNormalization(axis=-1, \n",
    "                                        momentum=0.99, \n",
    "                                        epsilon=0.001, center=True,\n",
    "                                        scale=True, beta_initializer='zeros', \n",
    "                                        gamma_initializer='ones', \n",
    "                                        moving_mean_initializer='zeros', \n",
    "                                        moving_variance_initializer='ones', \n",
    "                                        beta_regularizer=None, \n",
    "                                        gamma_regularizer=None, \n",
    "                                        beta_constraint=None, gamma_constraint=None)(X)\n",
    "    \n",
    "    \n",
    "    X = Dropout(0.5, noise_shape=None, seed=None)(X)\n",
    "    X = Dense(2)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(input=sentence_indices, output=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(fasttext_loc):\n",
    "    \"\"\"Loads fasttext vec file\n",
    "    Args:\n",
    "        fasttext_loc: path to vec file\n",
    "    Returns:\n",
    "        data : Dict[token:np.array] \n",
    "        vocab : list of tokens\n",
    "    \"\"\"\n",
    "    fin = io.open(fasttext_loc, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    vocab = []\n",
    "    data = {}\n",
    "    for line in tqdm(fin):\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        vocab.append(tokens[0])\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    return data,vocab\n",
    "\n",
    "def gen_word_index_maps(vocab):\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    for i in tqdm(range(len(vocab)), total = len(vocab)):\n",
    "        word_to_index[vocab[i]] = i\n",
    "        index_to_word[i] = vocab[i]\n",
    "    return word_to_index,index_to_word\n",
    "\n",
    "def create_embedding_layer(word_to_vec_map, word_to_index ):\n",
    "    vocab_length = len(word_to_index) +1\n",
    "    \n",
    "    emb_dim = word_to_vec_map['hurray'].shape[0]\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_length,emb_dim))\n",
    "    \n",
    "    for word, index in tqdm(word_to_index.items(), total= len(word_to_index)):\n",
    "        if word_to_vec_map[word].shape[0]==emb_dim:\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        else : ##weird dimention error\n",
    "            emb_matrix[index, :] = np.concatenate((pretrained_model[word],[0,]), axis=0)\n",
    "            \n",
    "\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_length, emb_dim, trainable = False)\n",
    "    \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def find_max_len(data):\n",
    "    tknz = TweetTokenizer()\n",
    "    max_len = 0\n",
    "    for sentence in data:\n",
    "        l = len(tknz.tokenize(sentence))\n",
    "        if  l > max_len:\n",
    "            max_len = l\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model,vocab = load_embedding(args.fast_text_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = gen_word_index_maps(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_layer = create_embedding_layer(fasttext_model, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_len = find_max_len(data_df.text)\n",
    "model = bilstm_model( (max_len,), embedding_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = len(X)\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    tknzr = TweetTokenizer()\n",
    "    for i in range(m):\n",
    "        sentence_words = tknzr.tokenize(X[i])\n",
    "        \n",
    "        sentence_words = sentence_words[:max_len].copy()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i,j] = word_to_index[w]\n",
    "            j=j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(data_df.text,word_to_index,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'NOT' : 0,\n",
    "    'HOF' : 1,\n",
    "}\n",
    "y_train_oh = to_categorical(data_df.task_1.map(label_map),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train_indices, y=y_train_oh , epochs = 30,batch_size=64 ,verbose=1,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
